{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    {\n",
    "        \"query\": \"What is the weather in Paris?\",\n",
    "        \"type\": \"informational\",  # Type: e.g., \"informational\", \"analytical\", \"creative\"\n",
    "        \"purpose\": \"general\",    # Purpose: e.g., \"general\", \"business\", \"technical\"\n",
    "        \"complexity\": 2          # Complexity: Numerical score, e.g., 1 (low) to 10 (high)\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Generate a report on sales trends for the last quarter.\",\n",
    "        \"type\": \"analytical\",\n",
    "        \"purpose\": \"business\",\n",
    "        \"complexity\": 7\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Write a poem about the moon.\",\n",
    "        \"type\": \"creative\",\n",
    "        \"purpose\": \"general\",\n",
    "        \"complexity\": 5\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain quantum computing in simple terms.\",\n",
    "        \"type\": \"technical\",\n",
    "        \"purpose\": \"educational\",\n",
    "        \"complexity\": 8\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the weather in Paris? | Score: 5.5\n",
      "Query: Generate a report on sales trends for the last quarter. | Score: 14.0\n",
      "Query: Write a poem about the moon. | Score: 12.5\n",
      "Query: Explain quantum computing in simple terms. | Score: 20.5\n"
     ]
    }
   ],
   "source": [
    "def calculate_query_score(query_dict):\n",
    "    type_weights = {\n",
    "        \"informational\": 1,\n",
    "        \"analytical\": 2,\n",
    "        \"creative\": 3,\n",
    "        \"technical\": 4\n",
    "    }\n",
    "    \n",
    "    purpose_weights = {\n",
    "        \"general\": 1,\n",
    "        \"business\": 2,\n",
    "        \"educational\": 3\n",
    "    }\n",
    "    \n",
    "    # Calculate the weighted score\n",
    "    score = (\n",
    "        type_weights.get(query_dict[\"type\"], 0) * 2 +\n",
    "        purpose_weights.get(query_dict[\"purpose\"], 0) * 1.5 +\n",
    "        query_dict[\"complexity\"]\n",
    "    )\n",
    "    return score\n",
    "\n",
    "# Example Usage\n",
    "for query in queries:\n",
    "    query[\"score\"] = calculate_query_score(query)\n",
    "    print(f\"Query: {query['query']} | Score: {query['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "        \"model_name\": \"GPT-3.5\",\n",
    "        \"providers\": [\"OpenAI\"],  # Providers or API sources\n",
    "        \"cost\": 5,  # Cost in terms of API usage (scale: 1 to 10)\n",
    "        \"performance\": 8  # Performance rating (scale: 1 to 10)\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"GPT-4\",\n",
    "        \"providers\": [\"OpenAI\"],\n",
    "        \"cost\": 8,\n",
    "        \"performance\": 10\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"LLaMA\",\n",
    "        \"providers\": [\"Meta\"],\n",
    "        \"cost\": 3,\n",
    "        \"performance\": 6\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"BERT\",\n",
    "        \"providers\": [\"Google\"],\n",
    "        \"cost\": 4,\n",
    "        \"performance\": 7\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GPT-3.5 | Score: 24.65\n",
      "Model: GPT-4 | Score: 27.2\n",
      "Model: LLaMA | Score: 21.25\n",
      "Model: BERT | Score: 22.95\n"
     ]
    }
   ],
   "source": [
    "def calculate_model_score(model_dict, query_complexity):\n",
    "    # Assign weights to cost and performance\n",
    "    cost_weight = 0.5\n",
    "    performance_weight = 1.5\n",
    "\n",
    "    # Inverse cost (lower cost is better) and performance (higher is better)\n",
    "    score = (\n",
    "        (10 - model_dict[\"cost\"]) * cost_weight +\n",
    "        model_dict[\"performance\"] * performance_weight\n",
    "    )\n",
    "    \n",
    "    # Factor in query complexity (higher complexity may need more expensive models)\n",
    "    adjusted_score = score * (1 + query_complexity / 10.0)\n",
    "    \n",
    "    return adjusted_score\n",
    "\n",
    "# Example Usage\n",
    "query_complexity = 7  # Assume the query complexity score is 7\n",
    "for model in models:\n",
    "    model[\"score\"] = calculate_model_score(model, query_complexity)\n",
    "    print(f\"Model: {model['model_name']} | Score: {model['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GPT-3.5 | Score: 22.475\n",
      "Model: GPT-4 | Score: 24.8\n",
      "Model: LLaMA | Score: 19.375\n",
      "Model: BERT | Score: 20.925\n",
      "Best model for the query: GPT-4\n"
     ]
    }
   ],
   "source": [
    "# Match query to the best model based on the scores\n",
    "def match_query_to_model(query_dict, models):\n",
    "    # Calculate query score\n",
    "    query_score = calculate_query_score(query_dict)\n",
    "    \n",
    "    # Calculate model scores and choose the best model\n",
    "    best_model = None\n",
    "    best_score = -float('inf')  # Start with a very low score\n",
    "    \n",
    "    for model in models:\n",
    "        model_score = calculate_model_score(model, query_score)\n",
    "        print(f\"Model: {model['model_name']} | Score: {model_score}\")\n",
    "        \n",
    "        if model_score > best_score:\n",
    "            best_score = model_score\n",
    "            best_model = model\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "# Example: Match the first query in the dictionary to the best model\n",
    "best_model = match_query_to_model(queries[0], models)\n",
    "print(f\"Best model for the query: {best_model['model_name']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Rankings Based on Score:\n",
      "Model B: Score = 0.810\n",
      "Model A: Score = 0.750\n",
      "Model C: Score = 0.740\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the model attributes (normalized values for each model)\n",
    "models = {\n",
    "    'Model A': {'accuracy': 0.85, 'speed': 0.9, 'cost': 0.4, 'scalability': 0.8, 'latency': 0.7},\n",
    "    'Model B': {'accuracy': 0.95, 'speed': 0.7, 'cost': 0.7, 'scalability': 0.9, 'latency': 0.6},\n",
    "    'Model C': {'accuracy': 0.8, 'speed': 0.8, 'cost': 0.5, 'scalability': 0.85, 'latency': 0.75}\n",
    "}\n",
    "\n",
    "# Define the normalization min and max values for each metric (based on domain knowledge or experimentation)\n",
    "min_values = {'accuracy': 0, 'speed': 0, 'cost': 0, 'scalability': 0, 'latency': 0}\n",
    "max_values = {'accuracy': 1, 'speed': 1, 'cost': 1, 'scalability': 1, 'latency': 1}\n",
    "\n",
    "# Define the weights for each metric (total should be 1)\n",
    "weights = {'accuracy': 0.4, 'speed': 0.2, 'cost': 0.2, 'scalability': 0.1, 'latency': 0.1}\n",
    "\n",
    "# Normalize function for each metric\n",
    "def normalize(value, min_value, max_value):\n",
    "    return (value - min_value) / (max_value - min_value) if max_value != min_value else 0\n",
    "\n",
    "# Function to calculate the model score\n",
    "def calculate_model_score(model, min_vals, max_vals, metric_weights):\n",
    "    score = 0\n",
    "    for metric, weight in metric_weights.items():\n",
    "        normalized_value = normalize(model[metric], min_vals[metric], max_vals[metric])\n",
    "        score += weight * normalized_value\n",
    "    return score\n",
    "\n",
    "# Calculate the score for each model\n",
    "model_scores = {}\n",
    "for model_name, model_values in models.items():\n",
    "    model_scores[model_name] = calculate_model_score(model_values, min_values, max_values, weights)\n",
    "\n",
    "# Sort models by score in descending order\n",
    "sorted_models = sorted(model_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display the sorted models with their scores\n",
    "print(\"Model Rankings Based on Score:\")\n",
    "for model_name, score in sorted_models:\n",
    "    print(f\"{model_name}: Score = {score:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
